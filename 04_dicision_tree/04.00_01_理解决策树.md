* [理解决策树](https://zhuanlan.zhihu.com/p/37954086)

    * 基本概念：
        * 决策树是分段线性函数但不是线性函数，它具有非线性建模的能力。只要划分的足够细，
        分段常数函数可以逼近闭区间上任意函数到任意指定精度，因此决策树在理论上可以对任意复杂度的数据进行分类或者回归。

    * 分裂

        * 定义不纯度的指标：当样本都属于某一类时不纯度为0；当样本均匀的属于所有类时不纯度最大。满足这个条件的有熵不纯度，Gini不纯度，以及误分类不纯度;

            * 熵不纯度

            ![img](../readme/决策树_熵不纯度.jpg)

            * Gini不纯度：当样本属于某一类时Gini不纯度的值最小，此时最小值为0；当样本均匀的分布与每一类时Gini不纯度的值最大；

            ![img](readme/决策树_Gini不纯度.jpg)

            * 样本集的误分类不纯度定义为

              ![img](readme/决策树_样本分类不纯度.jpg)

        * 分类：计算每个特征分裂后的熵和节点的熵的变化程度，选择信息增益最大的特征作为分裂特征；

        * 回归： 根据方差最大的分裂，计算分裂后标签的方差和分裂前标签的方差；

    * 何时停止  
        * 对于分类问题，当节点的样本都属于同一类型时停止，但是这样可能会导致树的节点过多、深度过大，产生过拟合问题。
        * 另一种方法是当节点中的样本数小于一个阀值时停止分裂;

    * 叶子节点值的设定：
        * 如果是分类树，则叶子节点的值设置成本节点的训练样本集中出现概率最大的那个类；
        * 如果是回归树，则设置为本节点训练样本标签值的均值。

    * 剪枝
        * 预剪枝可以通过限定树的高度，节点的训练样本数，分裂所带来的纯度提升的最小值来来实现，
        * 后剪枝： 计算剪枝前剪枝后的代价函数，主要是为了防止过拟合。
        ![](readme/决策树_loss_04.jpg)
        * 第一步先训练出T0，然后用上面的方法逐步剪掉树的所有非叶子节点，直到只剩下根节点得到剪枝后的树序列。
        这一步的误差计算采用的是训练样本集。
        * 第二步根据真实误差值从上面的树序列中挑选出一棵树作为剪枝后的结果。这可以通过交叉验证实现，
        用交叉验证的测试集对上一步得到的树序列的每一棵树进行测试，得到这些树的错误率，然后根据错误率选择最佳的树作为剪枝后的结果。