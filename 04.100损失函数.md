## 损失函数

* [如何理解决策树的损失函数?](https://www.zhihu.com/question/34075616)
    * 分裂的时候不需要损失函数，剪枝的时候需要；
    * 显而易见，衡量“决策树完全一致”的说明的是【确定性】吧？即确定性越强的损失函数最优值，说明学习方法效果越好。
    即损失函数要满足：熵值越小函数结果越好。
    * 显而易见，对于“决策树而言”，分类确定性越强的，损失就越小吧。
    * 不能完全按照极端的情况去处理，毕竟如果每个叶节点一个样本，咱们的分类也没那么多啊，是吧？这如何是好？机器学习的思路，
    或者说统计学的思路咱们就可以借鉴了，既然咱们不能保证每个最小，那咱们就求他们和的极限，因为决策树的熵都是大于0的，
    所以熵的和最小，就可以保证每个个体都是相对最小的，就可以保证整体最优。所以咱们将决策树的经验损失函数定义为，所有叶节点熵之和：
    ![](readme/决策树_loss_01.png)， N(t)是决策树在该节点的样本数,
    ![](readme/决策树_loss_02.png)
    *  交叉熵对误分率是一个大的改进, 至少两方面相比误分率是有优势的,一是误分率是不可微的(可微是很重要地),
    另外一个是效果要比直接误分率更好(如果效果更差, 当然不会被引进来了).
    ![](readme/决策树_loss_03.png)
    * 机器学习是一门思考的科学，万变不离其中，多考虑基础，多思考思路，总思维、轻记忆。千万不能死记硬背，碰到迷茫的问题的时候，
    站在解决问题的角度去思考一个方案，你往往就能够明白这些书本中觉得“理所当然的公式”的意义了。
    * 判别模型只是给定输入的一个输出，而生成模型也会带有生成模型的过程，所以判别模型容易过拟合，对于缺失数据无能为力，
    生成模型不容易过拟合，过于缺失数据也有一定的分类能力。
    * 为什么要乘以Nt: 这个叶子节点内部取k个类的不确定度，注意是节点【内部】的不确定度，每个叶子节点可以看作是独立的，
    既然是内部的事情，凭什么暴力的将各个内部的不确定度相加，我们至少到同一个级别的平台再加吧。所以使用节点元素的个数
    做一个加权。如果概率看做频率，那么这里就成了某一类出现的次数。
    
