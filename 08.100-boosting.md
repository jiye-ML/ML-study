## what

### 起源

* Valiant和 Kearns首次提出了 PAC学习模型中弱学习算法和强学习算法的等价性问题,即任意给定仅比随机猜测略好的弱学习算法 ,是否可以将其提升为强学习算法 ? 如果二者等价 ,那么只需找到一个比随机猜测略好的弱学习算法就可以将其提升为强学习算法 ,而不必寻找很难获得的强学习算法。
* 1990年, Schapire最先构造出一种多项式级的算法 ,对该问题做了肯定的证明 ,这就是最初的 Boosting算法。
  一年后 ,Freund提出了一种效率更高的Boosting算法。但是,这两种算法存在共同的实践上的缺陷 ,那就是都要求事先知道弱学习算法学习正确的下限。
* 1995年 , Freund和 Schapire改进了Boosting算法 ,提出了 AdaBoost 算法该算法效率和 Freund于 1991年提出的 Boosting算法几乎相同 ,但不需要任何关于弱学习器的先验知识 ,因而更容易应用到实际问题当中。
* 之后， Freund和 Schapire进一步提出了改变 Boosting投票权重的 AdaBoost .M1，AdaBoost . M2等算法 ,在机器学习领域受到了极大的关注。

### 发展

* Boosting算法是一种把若干个分类器整合为一个分类器的方法，也就是一种**集成分类方法（Ensemble Method）**。比较简单的集成分类方法在boosting之前出现过boostrapping和bagging方法，我们先简要介绍一下这两个方法。

#### 已有的方法

* Boostrapping:
    1. 重复地从一个样本集合D中采n个样本
    2. 针对每次采样的子样本，进行统计学习，获得假设$H_{i}​$
    3. 将若干个假设进行组合，形成最终的假设$H_{final}$
    4. 将最终的假设用于具体的分类任务
* Bagging
    1. 从整体样本集合中抽样产生不同的训练集并训练弱分类器
    2. 用分类器对分类进行投票，最终的分类结果是弱分类器投票的优胜结果

### 已有方法的不足

* 上述这两种方法，都只是将分类器进行简单的组合，实际上，并没有发挥出分类器组合的威力来。直到1989年，Freund与Schapire提出了一种可行的将弱分类器组合为强分类器的方法。并由此而获得了2003年的哥德尔奖（Godel price）。

* Schapire还提出了一种早期的boosting算法，其主要过程如下：
  1. 从样本整体集合D中，不放回的随机抽样n1<n 个样本，得到集合 D1训练弱分类器C1
  2. 从样本整体集合D中，抽取 n2<n 个样本，其中合并进一半被 C1 分类错误的样本。得到样本集合 D2训练弱分类器C2
  3. 抽取D样本集合中，C1 和C2分类不一致样本，组成D3训练弱分类器C3
  4. 用三个分类器做投票，得到最后分类结果


## why good

### 会不会过拟合

* 不会，因为根据置信度，boosting会让每个样本的置信度更高，所以会形成margin，
* margin的优点就是可以防止过拟合


# 
  

  

### reference

* [Boosting](https://blog.csdn.net/xiaohukun/article/details/78189281)
* [Boosting](https://zhuanlan.zhihu.com/p/26215100)
* [the-boosting-approach-to-machine-learning-an-overview](paper/the-boosting-approach-to-machine-learning-an-overview.pdf)