## where

### 优缺点

* 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整
* 缺点：对离群点敏感
* 使用数据类型：数值型和标称型数据

### 动机

* 尽管委员会中每个成员只提供一些不成熟的判断,但整个委员会却产生较为准确的决策。
* AdaBoost通过组合多个弱学习器来解决学习问题。给定训练数据,弱学习算法(如决策树)可以训练产生弱学习器,这些弱学习器只需要比随机猜测的准确率好一些。
* 用不同的训练数据训练可以得到不同的弱学习器。这些弱学习器作为委员会成员,共同决策。为了从不同的弱学习器中聚集”智慧”,AdaBoost解决了如下两个问题:首先,如何选择一组有不同优缺点的弱学习器,使得它们可以相互弥补不足。其次,如何组合弱学习器的输出以获得整体的更好的决策表现。
* 为了解决第一个问题,AdaBoost让每一个新加入的弱学习器都体现出一些新的数据中的模式。为了实现这一点,AdaBoost为每一个训练样本维护一个权重分布。即对任意一个样本![x_i](https://www.zhihu.com/equation?tex=x_i)都有一个分布![D(i)](https://www.zhihu.com/equation?tex=D%28i%29)与之对应,表示这个样本的重要性。当衡量弱学习器的表现时,AdaBoost会考虑每个样本的权重。权重较大的误分类样本会比权重较小的误分类样本贡献更大的训练错误率。为了获得更小的加权错误率,弱分类器必须更多的聚焦于高权重的样本,保证对它们准确的预测。通过修改样本的权重![D(i)](https://www.zhihu.com/equation?tex=D%28i%29)就可以引导弱学习器学习训练样本的不同部分。

## how

### 算法描述

* AdaBoost分多轮训练,在训练的第![t](https://www.zhihu.com/equation?tex=t)轮,我们更新样本的权重到![D_t](https://www.zhihu.com/equation?tex=D_t)并训练一个弱学习器,使得该学习器在权重分布![D_t](https://www.zhihu.com/equation?tex=D_t)上产生最小的加权训练误差。在第一轮中,所有样本权重相同,为 ![\frac{1}{n\_samples}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%5C_samples%7D) 。
* 在之后的每一轮中,我们提高误分类样本的权重,降低准确分类样本的权重。这样我们就使得每一轮的弱学习器都更多的聚焦于上一轮中较难被准确分类的样本。
* 现在，我们获得了一组已训练的拥有不同优缺点的弱学习器，如何有效的组合它们，使得相互优势互补来产生更准确的整体预测效果？每一个弱学习器是用不同的权重分布训练出来的，我们可以看做给不同的弱学习器分配了不同的任务，每个弱学习器都尽力完成给定的任务。直觉上看，当我们要把每个弱学习器的判断组合到最终的预测结果中时，如果弱学习器在之前的任务中表现优异，我们会更多的相信它，相反，如果弱学习器在之前的任务中表现较差，我们就更少的相信它。换句话说,我们会加权地组合弱学习器，给每个弱学习器赋予一个表示可信程度的值![a_t](https://www.zhihu.com/equation?tex=a_t) ，这个值取决于它在被分配的任务中的表现，表现越好![a_t](https://www.zhihu.com/equation?tex=a_t)越大，反之越小。

### 算法展示

* 我们先来看一个例子(Figure 1)。我们有10个样本点被标记为+1或者-1。假设我们的弱学习器是一条水平的或者竖直的线。很容易看出没有任何一条线可以完美地分类这10个样本点。正如我们即将看到的那样，AdaBoost可以通过组合弱学习器，生成一个在样本中完美分类的分类模型。

* 在第一轮，每个样本点具有相同的权重，弱学习器h1通过最小化加权误差得到。有三个样本点被错误分类(圆圈中的点)。在第二轮,三个样本点被赋予更大的权重(权重用样本点的大小来表示)，而其他的点权重被降低。这样h2就更多的学习错误分类点的特征。但是h2 错误分类了具有较小权重的其他样本点。在第三轮，h3只错误分类了三个具有非常低的权重的样本点。h1, h2, h3加权投票后 得到的组合模型准确分类了所有的样本点(Figure 2)。

![image-20190320085522847](/Volumes/jiye-学习/AI/ML-study/readme/08.Adaboost_算法展示.png)

### 算法细节

给定![(x_1, y_1), (x_2, y_2),...,(x_m,y_m), x_i \in\chi, y_i \in \{-1, 1\}](https://www.zhihu.com/equation?tex=%28x_1%2C+y_1%29%2C+%28x_2%2C+y_2%29%2C...%2C%28x_m%2Cy_m%29%2C+x_i+%5Cin%5Cchi%2C+y_i+%5Cin+%5C%7B-1%2C+1%5C%7D)

> 1.初始化：![for\ i=1,2,3...m\ \ \ D_1(i) = 1/m](https://www.zhihu.com/equation?tex=for%5C+i%3D1%2C2%2C3...m%5C+%5C+%5C+D_1%28i%29+%3D+1%2Fm)
> 2.![for\ t=1,2...,T:](https://www.zhihu.com/equation?tex=for%5C+t%3D1%2C2...%2CT%3A)
> 使用![D_t ](https://www.zhihu.com/equation?tex=D_t%0A)训练弱学习器，获得弱学习器![h_t:\chi\rightarrow\{-1,1\}](https://www.zhihu.com/equation?tex=h_t%3A%5Cchi%5Crightarrow%5C%7B-1%2C1%5C%7D)
> 训练目标：最小化加权误差![\epsilon_t=Pr_{i\backsim D_t}[h_t(x_t) \neq y_i]](https://www.zhihu.com/equation?tex=%5Cepsilon_t%3DPr_%7Bi%5Cbacksim+D_t%7D%5Bh_t%28x_t%29+%5Cneq+y_i%5D)
> 计算模型的权重：![a_t = \frac{1}{2} \ln(\frac{1-\epsilon_t}{\epsilon_t})](https://www.zhihu.com/equation?tex=a_t+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cln%28%5Cfrac%7B1-%5Cepsilon_t%7D%7B%5Cepsilon_t%7D%29)
> 更新样本分布：![D_{t+1} = \frac{D_t(i)}{Z_t}\times\left\{\begin{array}{lc} e^{-a_t} & if\ h_t(x_i) = y_i\\ e^{a_t} & if\ h_t(x_i) \neq y_i \end{array}\right.](https://www.zhihu.com/equation?tex=D_%7Bt%2B1%7D+%3D+%5Cfrac%7BD_t%28i%29%7D%7BZ_t%7D%5Ctimes%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Blc%7D%0Ae%5E%7B-a_t%7D+%26+if%5C+h_t%28x_i%29+%3D+y_i%5C%5C%0Ae%5E%7Ba_t%7D+%26+if%5C+h_t%28x_i%29+%5Cneq+y_i%0A%5Cend%7Barray%7D%5Cright.)
> (![Z_t](https://www.zhihu.com/equation?tex=Z_t)是归一化系数，保证![D_t](https://www.zhihu.com/equation?tex=D_t)为一个概率分布)
> 3.输出最终结果:![H(x) = sign(\Sigma^T_{t=1}a_th_t(x))](https://www.zhihu.com/equation?tex=H%28x%29+%3D+sign%28%5CSigma%5ET_%7Bt%3D1%7Da_th_t%28x%29%29)

### 训练误差

令![\gamma = \frac{1}{2} -\epsilon_t](https://www.zhihu.com/equation?tex=%5Cgamma+%3D+%5Cfrac%7B1%7D%7B2%7D+-%5Cepsilon_t),可以证明集成的分类器![H(x)](https://www.zhihu.com/equation?tex=H%28x%29)在![D_1 ](https://www.zhihu.com/equation?tex=D_1%0A)上的训练误差满足：

![image-20190320090111219](/Volumes/jiye-学习/AI/ML-study/readme/08.Adaboost-误差.png)

证明如下

![image-20190320090216798](/Volumes/jiye-学习/AI/ML-study/readme/08.Adaboost_误差-02.png)

因此，AdaBoost训练最终会出现两种情况：一种是训练误差降低到零。另一种是![\gamma_t=0(\epsilon_t=0.5)](https://www.zhihu.com/equation?tex=%5Cgamma_t%3D0%28%5Cepsilon_t%3D0.5%29)，算法阻塞：这样的分布使得每一个后续的弱分类器都只有0.5的准确率




## reference 

* [adaboost研究](paper/adaboost-研究.pdf)