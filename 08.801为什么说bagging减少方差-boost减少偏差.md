
* [为什么说bagging是减少variance，而boosting是减少bias?](https://www.zhihu.com/question/26760839)
    * bagging：比如随机森林， 利用不同特征学习，这样每个学习器对于整体样本都是欠拟合的；
        * 单个强学习器但由于部分数据来说学习的模型不一样
        * 多个分类器看到的数据不一样，减少了每个数据对模型的影响
        * 关键还是防止过拟合，减少输出对于某一个数据的依赖；
    * boosting：
        * 学习器之间就是为了减少bias，是依赖的
        * 所以不能减少模型对于某个数据的依赖，但是每个数据的偏差是减少的，不然就要继续学习。
