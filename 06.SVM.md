## 本章主要内容
* why svm
    * 减少计算量， 只有支持向量起作用；
    * 逻辑回归中所有点都要算一遍
* how solve svm问题
    * 两种解法：
    1. 正规方程：
        * 转换为对偶问题
        * 使用SMO解决对偶问题，获得原问题的解
        * 需要目标函数，不需要损失函数
    2. 梯度下降：
        * hingloss=max(1-z, 0): 只有间隔内部的点起作用


## 写在前面

* 优点：泛化错误率低， 计算开销不大， 结果易解释；
* 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题
* 使用数据类型： 数值型和标称型数据

* SVM有很多种实现，最流行的一种实现是： 序列最小优化(Sequential Minimal Optimization, SMO)算法。
    * SMO算法是将大优化问题分解的结果与将它们作为整体来求解的。这些小优化问题来求解；
    * 工作原理：每次循环选择两个 alpha 进行优化处理，一旦找出一对合适的 alpha，那么就增大一个同时减少一个。
        * 这里指的合适必须要符合一定的条件
        1. 这两个 alpha 必须要在间隔边界之外；
        2. 这两个 alpha 还没有进行过区间化处理或者不在边界上。

* [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/macyang/article/details/38782399)

    * 它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，
    最终可转化为一个凸二次规划问题的求解。
    ![SVM_超平面方程](readme/SVM_超平面方程.png)

* [第6章 支持向量机](https://github.com/apachecn/MachineLearning/blob/master/docs/6.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md)

    * 松弛变量：允许一定的样本在间隔内，但对于这部分样本做惩罚，约束

* [机器学习有很多关于核函数的说法，核函数的定义和作用是什么？](https://www.zhihu.com/question/24627666)

    * 核函数！=内积！=映射！=相似度，核函数是一种表征映射、实现内积逻辑关系且降低计算复杂度的一类特殊函数(满足Mercer's condition)！

* [逻辑回归 vs 决策树 vs 支持向量机](https://www.jianshu.com/p/fff29251a13c)

* [SVM（支持向量机）属于神经网络范畴吗？](https://www.zhihu.com/question/22290096)
    * 这两者都可以看做是一个loss function+f(x)的形式。其中x是输入。
        1. 当这个loss function是hinge loss和maximum margin，f(x)是线性函数w*x+b（其中w和b就是喜闻乐见的SVM权重和bias）的时候，
        那么这个结构就是线性SVM
        2. 当这个loss function是cross categorical entropy（或者其他什么的奇奇怪怪的函数），f(x)是一堆堆叠的非线性隐藏层的时候，
        那么这个结构就是神经网络。



## 手写笔记

![](readme/svm_手写笔记01.jpg)

> 上面节alpha的过程就是SMO算法。

![](readme/svm_手写笔记02.jpg)
![](readme/svm_手写笔记03.jpg)
![](readme/svm_手写笔记04.jpg)
![](readme/svm_手写笔记05.jpg)
![](readme/svm_手写笔记06.jpg)
![](readme/svm_手写笔记总结.jpg)





## svm 

* 支持向量的数目存在一个最优值，SVM的优点在于它能对数据进行高效分类，如果支持向量太少，就可能会得到一个很差的决策边界，
如果支持向量太多，也就相当于每次都利用真个数据集进行分类，这号给你分类方法称为knn。
* knn效果确实不错，但是需要保留所有的训练样本，而对于支持向量机而言，其需要保留的样本少了很多（支持向量）。
* 支持向量机是一种分类器。之所以叫做机是因为它会产生一个二值决策结果，即它是一种决策机。支持向量机的泛化能力错误率较低，
也就是说它具有良好的学习能力且学到的结果具有良好的推广性，这些优点使得支持向量机十分流行。




## 对偶问题

* [拉格朗日乘子法](https://github.com/jiye-algorithm/math/blob/master/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95.jpg)





## SMO算法

### SMO中启发式选择变量

* 在SMO算法中，我们每次需要选取一对 \alpha 来进行优化，
通过启发式的选取我们可以更高效的选取待优化的变量使得目标函数下降的最快。
针对第一个 \alpha_1 和第二个\alpha_2 Platt SMO采取不同的启发式手段。

#### 第一个变量的选择

* 第一个变量的选择为外循环，与之前便利整个 \alpha 列表不同，在这里我们在整个样本集和非边界样本集间进行交替:

![](readme/SMO_启发式_第一个alpha选择.png)
![](readme/SMO_启发式_第二个alpha选择.png)
![](readme/kkt条件允许一定的误差.png)


### 参考文献
* [支持向量机(SVM)（五）-- SMO算法详解](https://blog.csdn.net/u011067360/article/details/26503719)
* [机器学习算法实践-SVM中的SMO算法](https://zhuanlan.zhihu.com/p/29212107)
* [支持向量机系列（5）——SMO算法解对偶问题](https://zhuanlan.zhihu.com/p/28299882)
* [机器学习算法实践-Platt SMO和遗传算法优化SVM](https://zhuanlan.zhihu.com/p/30173372)





## 核函数

* 在低维空间度量任意两个样本之间的相似性，作为高维空间样本对于点的內积
* 可以把和函数想象成包装器或者接口，它能把数据从某个很难处理的形式转换成另一个比较容易处理的形式
* 我们可以在高维空间中解决线性问题，这也等价于在低维空间中解决非线性问题。
* SVM优化中一个特别好的地方就是，所有的运算都可以写成內积形式，可以把內积运算替换成核函数，而不必简化处理，
将內积转换成核函数的形式被称为核技巧。





## 杂谈

### 学习本章使用的内容

* 优达学城 《机器学习基础》
    * [课程](https://classroom.udacity.com/courses/ud120/lessons/2252188570/concepts/30294285900923)
    * [代码](https://github.com/udacity/ud120-projects)
* 《机器学习》周志华
    * 基本理论都能估计到，但是写的比较高深，需要一定的辅助，
    * 可以选择博客，补充概念
* 《python大战机器学习》 
    * 第一是对西瓜书的补充，第二是API实现，学习已有的函数的用法
* 吴恩达机器学习视频 SVM一节
    * 理解了很多的概念，算得上一个层次的视角，但是需要一定的基础
