## 本章主要内容
* why svm
    * 线性可分， 可用逻辑回归替代
    * 线性不可分问题，使用核函数
* how solve svm问题
    * 转换为对偶问题
    * 使用SMO解决对偶问题，获得原问题的解
    

## 写在前面

* [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/macyang/article/details/38782399)
    * 它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，
    最终可转化为一个凸二次规划问题的求解。
    ![SVM_超平面方程](readme/SVM_超平面方程.png)

* [第6章 支持向量机](https://github.com/apachecn/MachineLearning/blob/master/docs/6.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md)
    * 松弛变量：允许一定的样本在间隔内，但对于这部分样本做惩罚，约束

* [机器学习有很多关于核函数的说法，核函数的定义和作用是什么？](https://www.zhihu.com/question/24627666)
    * 核函数！=内积！=映射！=相似度，核函数是一种表征映射、实现内积逻辑关系且降低计算复杂度的一类特殊函数(满足Mercer's condition)！

* [逻辑回归和SVM的区别是什么？各适用于解决什么问题？](https://www.zhihu.com/question/24904422)
    * 也就是说，它们的区别就在于逻辑回归采用的是 log loss（对数损失函数），svm采用的是hinge loss: E(z) = max(0,1-z)。
    * 这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑 support vectors，
    也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，
    相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。
    * 其实，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
    SVM的处理方法是只考虑 support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，
    大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。???

* [逻辑回归 vs 决策树 vs 支持向量机](https://www.jianshu.com/p/fff29251a13c)
* [LR与SVM的异同](https://www.cnblogs.com/zhizhan/p/5038747.html)
    * 不同的loss function代表了不同的假设前提，也就代表了不同的分类原理，也就代表了一切！！！
    * 简单来说，​逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值，
    * 支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面
    ![SVM_vs_逻辑回归](readme/SVM_vs_逻辑回归.png)
    * 不同点三点
        1. 本质上是其loss function不同。
        2. 支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。
        3. 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。
        4. 线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响
        5. SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！
        而LR必须另外在损失函数上添加正则项！！！

* [SVM（支持向量机）属于神经网络范畴吗？](https://www.zhihu.com/question/22290096)
    * 这两者都可以看做是一个loss function+f(x)的形式。其中x是输入。
        1. 当这个loss function是hinge loss和maximum margin，f(x)是线性函数w*x+b（其中w和b就是喜闻乐见的SVM权重和bias）的时候，
        那么这个结构就是线性SVM
        2. 当这个loss function是cross categorical entropy（或者其他什么的奇奇怪怪的函数），f(x)是一堆堆叠的非线性隐藏层的时候，
        那么这个结构就是神经网络。



## 手写笔记

![](readme/svm_手写笔记01.jpg)
![](readme/svm_手写笔记02.jpg)
![](readme/svm_手写笔记03.jpg)
![](readme/svm_手写笔记04.jpg)
![](readme/svm_手写笔记05.jpg)
![](readme/svm_手写笔记06.jpg)
![](readme/svm_手写笔记总结.jpg)





## svm 

* 支持向量的数目存在一个最优值，SVM的优点在于它能对数据进行高效分类，如果支持向量太少，就可能会得到一个很差的决策边界，
如果支持向量太多，也就相当于每次都利用真个数据集进行分类，这号给你分类方法称为knn。
* knn效果确实不错，但是需要保留所有的训练样本，而对于支持向量机而言，其需要保留的样本少了很多（支持向量）。
* 支持向量机是一种分类器。之所以叫做机是因为它会产生一个二值决策结果，即它是一种决策机。支持向量机的泛化能力错误率较低，
也就是说它具有良好的学习能力且学到的结果具有良好的推广性，这些优点使得支持向量机十分流行。




## 对偶问题

* [拉格朗日乘子法](https://github.com/jiye-algorithm/math/blob/master/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95.jpg)





## SMO算法

### SMO中启发式选择变量

* 在SMO算法中，我们每次需要选取一对 \alpha 来进行优化，
通过启发式的选取我们可以更高效的选取待优化的变量使得目标函数下降的最快。
针对第一个 \alpha_1 和第二个\alpha_2 Platt SMO采取不同的启发式手段。

#### 第一个变量的选择

* 第一个变量的选择为外循环，与之前便利整个 \alpha 列表不同，在这里我们在整个样本集和非边界样本集间进行交替:

![](svm/SMO_启发式_第一个alpha选择.png)
![](svm/SMO_启发式_第二个alpha选择.png)
![](svm/kkt条件允许一定的误差.png)


### 参考文献
* [支持向量机(SVM)（五）-- SMO算法详解](https://blog.csdn.net/u011067360/article/details/26503719)
* [机器学习算法实践-SVM中的SMO算法](https://zhuanlan.zhihu.com/p/29212107)
* [支持向量机系列（5）——SMO算法解对偶问题](https://zhuanlan.zhihu.com/p/28299882)
* [机器学习算法实践-Platt SMO和遗传算法优化SVM](https://zhuanlan.zhihu.com/p/30173372)





## 核函数

* 在低维空间度量任意两个样本之间的相似性，作为高维空间样本对于点的內积
* 可以把和函数想象成包装器或者接口，它能把数据从某个很难处理的形式转换成另一个比较容易处理的形式
* 我们可以在高维空间中解决线性问题，这也等价于在低维空间中解决非线性问题。
* SVM优化中一个特别好的地方就是，所有的运算都可以写成內积形式，可以把內积运算替换成核函数，而不必简化处理，
将內积转换成核函数的形式被称为核技巧。





## 杂谈

### 学习本章使用的内容

* 优达学城 《机器学习基础》
    * [课程](https://classroom.udacity.com/courses/ud120/lessons/2252188570/concepts/30294285900923)
    * [代码](https://github.com/udacity/ud120-projects)
* 《机器学习》周志华
    * 基本理论都能估计到，但是写的比较高深，需要一定的辅助，
    * 可以选择博客，补充概念
* 《python大战机器学习》 
    * 第一是对西瓜书的补充，第二是API实现，学习已有的函数的用法
* 吴恩达机器学习视频 SVM一节
    * 理解了很多的概念，算得上一个层次的视角，但是需要一定的基础
