* 弱学习：识别错误率小于1/2（即准确率仅比随机猜测略高的学习算法） 
* 强学习：识别准确率很高并能在多项式时间内完成的学习算法



## what

### 动机

* 在一个传统的机器学习任务中，面对不同的数据会采用不同的模型。
* 面对回归任务，我们可能会采用简单的线性回归，以保证模型的可解释性，但随着特征的增多，简单线性回归往往会出现多重共线性的问题。我们接下来会尝试多项式回归来解决数据的微弱的非线性倾向，但这样的回归方式也并不能处理复杂的非线性关系，而且随着特征的增多，多项式会变得非常臃肿，即便采用了岭回归和LASSO，以及弹性网，仍然无法有效解决高维空间的特征间复杂的非线性组合。我们可以继续尝试添加kernel的岭回归和支持向量机，但kernel的选取需要对数据的先验形式做出假设，参数的增多需要不断地去调试参数。如果你厌倦了这一方式，那么还可以尝试种下一棵决策树，但决策树（以及高斯过程）这样的非参数模型天然有着比参数模型更高的过拟合风险。
* 同样，面对分类任务，我们可以考虑朴素贝叶斯的方法，它面对属性值离散的数据就会变成一个计数工作。但它假设太过简单，从理论上来讲无法胜任特征之间存在相关性的数据（虽然在实践中，性能还不错），
* 如果我们想进一步地提高性能，那么就可以考虑广义线性模型中而来的logistic回归，它具备概率的框架，还具备很好的解释性，但对于特征空间线性不可分的数据往往会无能为力，
* 我们就可以想到kernel function的作用正是通过升维将线性不可分的数据变得线性可分，所以我们还会尝试添加kernel的支持向量机，但支持向量机参数较多难以调优，且解释性比较差。
* 接下来，我们为了非线性和解释性，还会采用多变量决策树的办法，但决策树面对多个输出结果且特征间存在复杂关系时，泛化性能也会不佳。

* 可以看出，没有任何一个模型可以胜任全部的机器学习任务。我们就会很自然的想到，面对同一个任务，可不可以将不同的模型结合起来，来达到想要的效果？比如，学习器A在某些样本上预测失误，但学习器B却可以将这些样本预测对，就好像面对很多科目的考试，数学成绩好的人去做数学卷子，语文好的人去做语文试题，如果只用一个人去做全部的试卷，那么考试分数都会很低，但把他们结合在一起，就有望达到我们想要的效果。
* 从数学的角度来说，每个模型都对应着不同的假设空间，而结合不同的模型会将假设空间扩大。这就是所谓的集成学习（ensemble learning），我们将不同的模型结合起来，而模型要满足一个最基本的条件，即集成学习中的每一个学习器尽可能的不同，因为两个数学成绩好的人去考试，无法优化语文成绩。为了获得差异较大的模型，主要有三种方法：
  1. 改变算法，对于同样的数据，可以采用不同的算法去构建模型。
  2. 改变数据，在相同的算法下，通过进入模型的数据不同来改变最终的模型。
  3. 改变参数，在相同的算法，相同的数据下，通过改变参数来改变的最终的模型。
* 注意，获得差异化的模型是集成学习的核心问题，没有之一，我们会在后面看到，这是集成学习性能提升的主要来源。而根据差异化模型的结合策略，又可以将集成学习的主要分为三类：
  1. bagging
  2. boosting
  3. stacking



