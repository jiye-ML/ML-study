目前的实际工作中，对特征选择没有这么重视，我们目前常用的方式是：

1、选择N多变量；

2、然后扔到xgboost和Randomforest模型去跑一边，然后查看一下变量重要性排序；一般性选择用gini增益查看变量重要性排序；

3、如果是数值型预测，也可以用xgboost加入lamda来进行变量的筛选；

![img](readme/12.902-效果图.jpg)![img]

4、根据这个重要性变量的排序，我们可以剔除掉那些不重要的变量，然后再重新预测一下模型，根据模型评估EvaluateMetric，看一下AUC等指标下降了多少；一般性都不是很重要的；

\---------------------------------------------------------------------------------------------------------------

以上方法是最快速有效的方法，其他的方法，REF，LASSO等方法都比价耗时耗力。因为很多的

特征选择的目的是为了规避特征之间的多重共线性，

导致模型效果欠拟合或者过拟合。而上述提供的方法能够很好的对抗特征之间的交互性，模型整体来说比较稳健。